version: "3.8"

services:
  # Step 1: Run Deployment
  deployment:
    build:
      context: .
      dockerfile: Dockerfile
    command: >
      bash -c "
      zenml init &&
      zenml integration install mlflow -y &&
      zenml experiment-tracker register mlflow_tracker_customer_churn_new --flavor=mlflow &&
      zenml model-deployer register mlflow_customer_churn_new --flavor=mlflow &&
      zenml stack register mlflow_stack_customer_churn_new -a default -o default -d mlflow -e mlflow_tracker_customer_churn_new --set &&
      zenml stack set mlflow_stack_customer_churn_new &&
      python3 run_pipeline.py&&
      python3 run_deployment.py
      "
    volumes:
      - .:/app
    working_dir: /app
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]  # Adjust URL for deployment health check
      interval: 10s
      retries: 3
      start_period: 5s
      timeout: 5s

  # Step 2: Run FastAPI service after Deployment is completed
  fastapi_service:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["uvicorn", "backend.fastapi_app:app", "--host", "0.0.0.0", "--port", "8001"]
    depends_on:
      - deployment
    volumes:
      - .:/app
    working_dir: /app
    ports:
      - "8001:8001"
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]  # Adjust URL for FastAPI health check
      interval: 10s
      retries: 3
      start_period: 5s
      timeout: 5s

  # Step 3: Run Streamlit UI after FastAPI service is up
  streamlit:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["streamlit", "run", "frontend/main.py"]
    depends_on:
      - fastapi_service
    volumes:
      - .:/app
    working_dir: /app
    ports:
      - "8501:8501"
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/health"]  # Adjust URL for Streamlit health check
      interval: 10s
      retries: 3
      start_period: 5s
      timeout: 5s
